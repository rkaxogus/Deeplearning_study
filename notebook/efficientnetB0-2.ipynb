{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 10ê°œì˜ í´ë˜ìŠ¤ ë°œê²¬: ['ê°€ë°©Â·ë°•ìŠ¤Â·ì›¨ê±´', 'ê³µêµ¬Â·ì•¡ì„¸ì„œë¦¬', 'ëƒ‰Â·ë‚œë°©ìš©í’ˆ', 'ëœí„´', 'ë§¤íŠ¸Â·ì¹¨ë‚­', 'ì‹ê¸°Â·ì£¼ë°©', 'ì „ê¸°Â·ì „ìê¸°ê¸°', 'í…Œì´ë¸”Â·ì²´ì–´', 'í…íŠ¸Â·íƒ€í”„', 'í™”ë¡œëŒ€Â·ë²„ë„ˆ']\n",
      "âœ… ì´ 13883ê°œì˜ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "=== Starting Epoch 1/30 ===\n",
      "Epoch 1/30: 100% completed\n",
      "Epoch 1/30 completed.\n",
      "Train Loss: 0.7227, Train Acc: 0.7688, Valid Loss: 0.5106, Valid Acc: 0.8408\n",
      "Created directory: ./models\n",
      "Model saved to ./models/category_model3.pth\n",
      "\n",
      "=== Starting Epoch 2/30 ===\n",
      "Epoch 2/30: 100% completed\n",
      "Epoch 2/30 completed.\n",
      "Train Loss: 0.2600, Train Acc: 0.9158, Valid Loss: 0.4078, Valid Acc: 0.8725\n",
      "Model saved to ./models/category_model3.pth\n",
      "\n",
      "=== Starting Epoch 3/30 ===\n",
      "Epoch 3/30: 100% completed\n",
      "Epoch 3/30 completed.\n",
      "Train Loss: 0.1537, Train Acc: 0.9504, Valid Loss: 0.4542, Valid Acc: 0.8740\n",
      "Model saved to ./models/category_model3.pth\n",
      "\n",
      "=== Starting Epoch 4/30 ===\n",
      "Epoch 4/30: 100% completed\n",
      "Epoch 4/30 completed.\n",
      "Train Loss: 0.1312, Train Acc: 0.9591, Valid Loss: 0.5093, Valid Acc: 0.8596\n",
      "\n",
      "=== Starting Epoch 5/30 ===\n",
      "Epoch 5/30: 100% completed\n",
      "Epoch 5/30 completed.\n",
      "Train Loss: 0.1127, Train Acc: 0.9655, Valid Loss: 0.5065, Valid Acc: 0.8704\n",
      "\n",
      "=== Starting Epoch 6/30 ===\n",
      "Epoch 6/30: 100% completed\n",
      "Epoch 6/30 completed.\n",
      "Train Loss: 0.0925, Train Acc: 0.9694, Valid Loss: 0.5188, Valid Acc: 0.8711\n",
      "\n",
      "=== Starting Epoch 7/30 ===\n",
      "Epoch 7/30: 100% completed\n",
      "Epoch 7/30 completed.\n",
      "Train Loss: 0.0765, Train Acc: 0.9751, Valid Loss: 0.5282, Valid Acc: 0.8754\n",
      "Model saved to ./models/category_model3.pth\n",
      "\n",
      "=== Starting Epoch 8/30 ===\n",
      "Epoch 8/30: 100% completed\n",
      "Epoch 8/30 completed.\n",
      "Train Loss: 0.0622, Train Acc: 0.9813, Valid Loss: 0.4524, Valid Acc: 0.8902\n",
      "Model saved to ./models/category_model3.pth\n",
      "\n",
      "=== Starting Epoch 9/30 ===\n",
      "Epoch 9/30: 100% completed\n",
      "Epoch 9/30 completed.\n",
      "Train Loss: 0.0565, Train Acc: 0.9814, Valid Loss: 0.4730, Valid Acc: 0.8894\n",
      "\n",
      "=== Starting Epoch 10/30 ===\n",
      "Epoch 10/30: 100% completed\n",
      "Epoch 10/30 completed.\n",
      "Train Loss: 0.0658, Train Acc: 0.9796, Valid Loss: 0.5189, Valid Acc: 0.8840\n",
      "\n",
      "=== Starting Epoch 11/30 ===\n",
      "Epoch 11/30: 100% completed\n",
      "Epoch 11/30 completed.\n",
      "Train Loss: 0.0602, Train Acc: 0.9806, Valid Loss: 0.5036, Valid Acc: 0.8804\n",
      "\n",
      "=== Starting Epoch 12/30 ===\n",
      "Epoch 12/30: 100% completed\n",
      "Epoch 12/30 completed.\n",
      "Train Loss: 0.0392, Train Acc: 0.9869, Valid Loss: 0.5120, Valid Acc: 0.8822\n",
      "\n",
      "=== Starting Epoch 13/30 ===\n",
      "Epoch 13/30: 100% completed\n",
      "Epoch 13/30 completed.\n",
      "Train Loss: 0.0377, Train Acc: 0.9886, Valid Loss: 0.5193, Valid Acc: 0.8855\n",
      "\n",
      "=== Starting Epoch 14/30 ===\n",
      "Epoch 14/30: 100% completed\n",
      "Epoch 14/30 completed.\n",
      "Train Loss: 0.0588, Train Acc: 0.9815, Valid Loss: 0.6258, Valid Acc: 0.8686\n",
      "\n",
      "=== Starting Epoch 15/30 ===\n",
      "Epoch 15/30: 100% completed\n",
      "Epoch 15/30 completed.\n",
      "Train Loss: 0.0518, Train Acc: 0.9838, Valid Loss: 0.5672, Valid Acc: 0.8758\n",
      "\n",
      "=== Starting Epoch 16/30 ===\n",
      "Epoch 16/30: 100% completed\n",
      "Epoch 16/30 completed.\n",
      "Train Loss: 0.0530, Train Acc: 0.9834, Valid Loss: 0.5553, Valid Acc: 0.8797\n",
      "\n",
      "=== Starting Epoch 17/30 ===\n",
      "Epoch 17/30: 100% completed\n",
      "Epoch 17/30 completed.\n",
      "Train Loss: 0.0404, Train Acc: 0.9866, Valid Loss: 0.5394, Valid Acc: 0.8707\n",
      "\n",
      "=== Starting Epoch 18/30 ===\n",
      "Epoch 18/30: 100% completed\n",
      "Epoch 18/30 completed.\n",
      "Train Loss: 0.0312, Train Acc: 0.9892, Valid Loss: 0.5420, Valid Acc: 0.8830\n",
      "\n",
      "=== Starting Epoch 19/30 ===\n",
      "Epoch 19/30: 100% completed\n",
      "Epoch 19/30 completed.\n",
      "Train Loss: 0.0236, Train Acc: 0.9926, Valid Loss: 0.5895, Valid Acc: 0.8783\n",
      "\n",
      "=== Starting Epoch 20/30 ===\n",
      "Epoch 20/30: 100% completed\n",
      "Epoch 20/30 completed.\n",
      "Train Loss: 0.0254, Train Acc: 0.9914, Valid Loss: 0.5949, Valid Acc: 0.8714\n",
      "\n",
      "=== Starting Epoch 21/30 ===\n",
      "Epoch 21/30: 100% completed\n",
      "Epoch 21/30 completed.\n",
      "Train Loss: 0.0319, Train Acc: 0.9893, Valid Loss: 0.5792, Valid Acc: 0.8902\n",
      "\n",
      "=== Starting Epoch 22/30 ===\n",
      "Epoch 22/30: 100% completed\n",
      "Epoch 22/30 completed.\n",
      "Train Loss: 0.0468, Train Acc: 0.9847, Valid Loss: 0.6760, Valid Acc: 0.8581\n",
      "\n",
      "=== Starting Epoch 23/30 ===\n",
      "Epoch 23/30: 100% completed\n",
      "Epoch 23/30 completed.\n",
      "Train Loss: 0.0394, Train Acc: 0.9876, Valid Loss: 0.6370, Valid Acc: 0.8736\n",
      "\n",
      "=== Starting Epoch 24/30 ===\n",
      "Epoch 24/30: 97% completed\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 177\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 63\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     61\u001b[0m train_loss, train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     64\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\classification\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\classification\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\classification\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\classification\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\classification\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[2], line 148\u001b[0m, in \u001b[0;36mCustomCategoryDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# âœ… ì´ë¯¸ì§€ íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ì—´ë¦¬ëŠ”ì§€ í™•ì¸\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš¨ ì˜¤ë¥˜: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ì—´ ìˆ˜ ì—†ìŒ (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\classification\\lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\classification\\lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "\n",
    "def main():\n",
    "    # ë°ì´í„° ê²½ë¡œ ì„¤ì • (ìµœìƒìœ„ í´ë”)\n",
    "    data_dir = r\"C:\\Users\\user\\OneDrive\\Desktop\\Resnet18-real\\data\\category_data\"\n",
    "    \n",
    "    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)\n",
    "    model_save_path = \"./models/category_model3.pth\"\n",
    "\n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    BATCH_SIZE = 128\n",
    "    IMG_SIZE = (224, 224)\n",
    "    EPOCHS = 30\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_CLASSES = 10  # í´ë˜ìŠ¤ ê°œìˆ˜ (ìë™ íƒìƒ‰)\n",
    "\n",
    "    # ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # âœ… ë°ì´í„°ì…‹ ë¡œë“œ (í•˜ìœ„ í´ë” ë‚´ ì´ë¯¸ì§€ ìë™ íƒìƒ‰)\n",
    "    dataset = CustomCategoryDataset(data_dir, transform=transform)\n",
    "    NUM_CLASSES = len(dataset.classes)  # í´ë˜ìŠ¤ ê°œìˆ˜ ìë™ ì„¤ì •\n",
    "\n",
    "    # âœ… ë°ì´í„° ë¶„í•  (80% í›ˆë ¨, 20% ê²€ì¦)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    valid_size = len(dataset) - train_size\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # âœ… EfficientNet-B0 ëª¨ë¸ ì •ì˜\n",
    "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scaler = GradScaler(\"cuda\")\n",
    "\n",
    "    # âœ… ê²€ì¦ ì •í™•ë„ê°€ ê°€ì¥ ë†’ì•˜ë˜ ëª¨ë¸ ì €ì¥\n",
    "    best_valid_accuracy = 0.0\n",
    "\n",
    "    # âœ… í•™ìŠµ ë£¨í”„\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n=== Starting Epoch {epoch + 1}/{EPOCHS} ===\")\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "            progress = int((batch_idx + 1) / len(train_loader) * 100)\n",
    "            print(f\"Epoch {epoch + 1}/{EPOCHS}: {progress}% completed\", end=\"\\r\")\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # âœ… Validation ë£¨í”„\n",
    "        valid_loss, valid_correct = 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "                valid_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        valid_accuracy = valid_correct / len(valid_loader.dataset)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS} completed.\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "            f\"Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # âœ… ëª¨ë¸ ì €ì¥ (ìµœê³  ì •í™•ë„ ì—…ë°ì´íŠ¸ ì‹œ)\n",
    "        if valid_accuracy > best_valid_accuracy:\n",
    "            best_valid_accuracy = valid_accuracy\n",
    "            save_model(model, optimizer, epoch + 1, train_loss, valid_loss, train_accuracy, valid_accuracy, model_save_path)\n",
    "\n",
    "    print(\"\\nTraining completed. Best model saved to:\", model_save_path)\n",
    "\n",
    "# âœ… CustomCategoryDatasetì—ì„œ ë°ì´í„° ë¡œë“œ ìƒíƒœ ì¶œë ¥ ì¶”ê°€\n",
    "class CustomCategoryDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # âœ… í´ë˜ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìµœìƒìœ„ í´ë”ë§Œ)\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"âœ… ì´ {len(self.classes)}ê°œì˜ í´ë˜ìŠ¤ ë°œê²¬: {self.classes}\")  # ë””ë²„ê¹… ì¶œë ¥\n",
    "\n",
    "        # âœ… í•˜ìœ„ í´ë” ë‚´ ì´ë¯¸ì§€ ìë™ íƒìƒ‰\n",
    "        for cls_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, cls_name)\n",
    "            found_images = 0\n",
    "            for root, _, files in os.walk(class_dir):\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif', 'tiff', 'webp')):\n",
    "                        self.image_paths.append(os.path.join(root, file))\n",
    "                        self.labels.append(self.class_to_idx[cls_name])\n",
    "                        found_images += 1\n",
    "\n",
    "            if found_images == 0:\n",
    "                print(f\"âš ï¸ {cls_name} í´ë” ë‚´ ì´ë¯¸ì§€ê°€ ì—†ìŒ (ë°ì´í„°ì…‹ ë¬¸ì œ ê°€ëŠ¥)\")\n",
    "\n",
    "        print(f\"âœ… ì´ {len(self.image_paths)}ê°œì˜ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ\")  # ë””ë²„ê¹… ì¶œë ¥\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # âœ… ì´ë¯¸ì§€ íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ì—´ë¦¬ëŠ”ì§€ í™•ì¸\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"ğŸš¨ ì˜¤ë¥˜: {img_path} ì—´ ìˆ˜ ì—†ìŒ ({e})\")\n",
    "            return None, None  # ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def save_model(model, optimizer, epoch, train_loss, valid_loss, train_accuracy, valid_accuracy, model_save_path):\n",
    "    \"\"\"ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    model_dir = os.path.dirname(model_save_path)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        print(f\"Created directory: {model_dir}\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'valid_loss': valid_loss,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'valid_accuracy': valid_accuracy,\n",
    "    }, model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‚¬ìš© ì¤‘ì¸ GPU: Quadro RTX 4000\n",
      "\n",
      "ğŸ”¥ ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ë¬¸ì œìˆëŠ” ë°ì´í„° ì°¾ê¸° ğŸ”¥\n",
      "\n",
      "âœ… ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ìì£¼ í‹€ë¦° í´ë˜ìŠ¤ (ìƒìœ„ 10ê°œ)\n",
      "Class ì‹ê¸°Â·ì£¼ë°©: 23ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "Class í™”ë¡œëŒ€Â·ë²„ë„ˆ: 18ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "Class ë§¤íŠ¸Â·ì¹¨ë‚­: 16ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "Class ëƒ‰Â·ë‚œë°©ìš©í’ˆ: 16ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "Class ê°€ë°©Â·ë°•ìŠ¤Â·ì›¨ê±´: 14ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "Class í…Œì´ë¸”Â·ì²´ì–´: 11ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "Class ì „ê¸°Â·ì „ìê¸°ê¸°: 10ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "Class ê³µêµ¬Â·ì•¡ì„¸ì„œë¦¬: 6ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "Class í…íŠ¸Â·íƒ€í”„: 4ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "Class ëœí„´: 4ë²ˆ ì˜¤ë‹µ ë°œìƒ\n",
      "\n",
      "âœ… ê²€ì¦ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë³„ ì •í™•ë„:\n",
      "Class ê°€ë°©Â·ë°•ìŠ¤Â·ì›¨ê±´: 95.10% ì •í™•ë„\n",
      "Class ê³µêµ¬Â·ì•¡ì„¸ì„œë¦¬: 95.20% ì •í™•ë„\n",
      "Class ëƒ‰Â·ë‚œë°©ìš©í’ˆ: 95.35% ì •í™•ë„\n",
      "Class ëœí„´: 95.56% ì •í™•ë„\n",
      "Class ë§¤íŠ¸Â·ì¹¨ë‚­: 96.99% ì •í™•ë„\n",
      "Class ì‹ê¸°Â·ì£¼ë°©: 94.38% ì •í™•ë„\n",
      "Class ì „ê¸°Â·ì „ìê¸°ê¸°: 96.34% ì •í™•ë„\n",
      "Class í…Œì´ë¸”Â·ì²´ì–´: 95.15% ì •í™•ë„\n",
      "Class í…íŠ¸Â·íƒ€í”„: 98.17% ì •í™•ë„\n",
      "Class í™”ë¡œëŒ€Â·ë²„ë„ˆ: 93.38% ì •í™•ë„\n",
      "\n",
      "âœ… ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ í™•ì‹ ë„ê°€ ë‚®ì€ ìƒ˜í”Œ (ìƒìœ„ 10ê°œ)\n",
      "\n",
      "ğŸ“„ ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ì˜¤ë‹µì´ ë§ì€ ìƒ˜í”Œ ì €ì¥ë¨: C:/Users/user/OneDrive/Desktop/Resnet18-real/csv\\misclassified_samples.csv\n",
      "ğŸ“„ ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ í™•ì‹ ë„ê°€ ë‚®ì€ ìƒ˜í”Œ ì €ì¥ë¨: C:/Users/user/OneDrive/Desktop/Resnet18-real/csv\\low_confidence_samples.csv\n",
      "ğŸ“„ í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì €ì¥ë¨: C:/Users/user/OneDrive/Desktop/Resnet18-real/csv\\classwise_accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# âœ… ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"âœ… ì‚¬ìš© ì¤‘ì¸ GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# âœ… ëª¨ë¸ ë° ë°ì´í„°ì…‹ ì„¤ì •\n",
    "data_dir = \"C:/Users/user/OneDrive/Desktop/Resnet18-real/data/category_data\"\n",
    "model_path = \"C:/Users/user/OneDrive/Desktop/Resnet18-real/model/best_category_model.pth\"\n",
    "csv_dir = \"C:/Users/user/OneDrive/Desktop/Resnet18-real/csv\"\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - valid_size\n",
    "_, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, len(dataset.classes))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# âœ… ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ìì£¼ í‹€ë¦° ë°ì´í„° ì°¾ê¸°\n",
    "def find_misclassified_samples(loader):\n",
    "    misclassified = []\n",
    "    class_misclassification = Counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = outputs.argmax(1)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                image_idx = idx * loader.batch_size + i\n",
    "                pred_label = dataset.classes[predicted[i].item()]\n",
    "                true_label = dataset.classes[labels[i].item()]\n",
    "\n",
    "                if pred_label != true_label:\n",
    "                    misclassified.append((image_idx, pred_label, true_label))\n",
    "                    class_misclassification[true_label] += 1\n",
    "\n",
    "    print(\"\\nâœ… ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ìì£¼ í‹€ë¦° í´ë˜ìŠ¤ (ìƒìœ„ 10ê°œ)\")\n",
    "    for label, count in class_misclassification.most_common(10):\n",
    "        print(f\"Class {label}: {count}ë²ˆ ì˜¤ë‹µ ë°œìƒ\")\n",
    "\n",
    "    return misclassified\n",
    "\n",
    "# âœ… í´ë˜ìŠ¤ë³„ ì •í™•ë„ í™•ì¸\n",
    "def compute_classwise_accuracy(loader):\n",
    "    class_correct = Counter()\n",
    "    class_total = Counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = outputs.argmax(1)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                true_label = labels[i].item()\n",
    "                class_total[true_label] += 1\n",
    "                if predicted[i] == true_label:\n",
    "                    class_correct[true_label] += 1\n",
    "\n",
    "    class_accuracy = {}\n",
    "    print(\"\\nâœ… ê²€ì¦ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë³„ ì •í™•ë„:\")\n",
    "    for label in sorted(class_total.keys()):\n",
    "        accuracy = class_correct[label] / class_total[label] if class_total[label] > 0 else 0\n",
    "        class_accuracy[dataset.classes[label]] = accuracy\n",
    "        print(f\"Class {dataset.classes[label]}: {accuracy:.2%} ì •í™•ë„\")\n",
    "\n",
    "    return class_accuracy\n",
    "\n",
    "# âœ… í™•ì‹ ë„ê°€ ë‚®ì€ ìƒ˜í”Œ ì°¾ê¸°\n",
    "def find_low_confidence_samples(loader, threshold=0.3):\n",
    "    low_confidence_samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            confidence, predicted = probabilities.max(1)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                image_idx = idx * loader.batch_size + i\n",
    "                if confidence[i].item() < threshold:  # ğŸ”¥ í™•ì‹ ë„ê°€ ë‚®ì€ ìƒ˜í”Œ ì €ì¥\n",
    "                    low_confidence_samples.append((image_idx, dataset.classes[predicted[i].item()], dataset.classes[labels[i].item()], confidence[i].item()))\n",
    "\n",
    "    print(\"\\nâœ… ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ í™•ì‹ ë„ê°€ ë‚®ì€ ìƒ˜í”Œ (ìƒìœ„ 10ê°œ)\")\n",
    "    for i, (image_idx, pred_label, true_label, conf) in enumerate(low_confidence_samples[:10]):\n",
    "        print(f\"ì´ë¯¸ì§€ {image_idx}: ì˜ˆì¸¡={pred_label}, ì‹¤ì œ={true_label}, í™•ì‹ ë„={conf:.2f}\")\n",
    "\n",
    "    return low_confidence_samples\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "print(\"\\nğŸ”¥ ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ë¬¸ì œìˆëŠ” ë°ì´í„° ì°¾ê¸° ğŸ”¥\")\n",
    "misclassified_valid = find_misclassified_samples(valid_loader)\n",
    "class_accuracy = compute_classwise_accuracy(valid_loader)\n",
    "low_confidence_valid = find_low_confidence_samples(valid_loader)\n",
    "\n",
    "# âœ… CSVë¡œ ì €ì¥ (ë¬¸ì œìˆëŠ” ë°ì´í„° ì •ë¦¬)\n",
    "misclassified_df = pd.DataFrame(misclassified_valid, columns=[\"Image Index\", \"Predicted Label\", \"True Label\"])\n",
    "low_confidence_df = pd.DataFrame(low_confidence_valid, columns=[\"Image Index\", \"Predicted Label\", \"True Label\", \"Confidence\"])\n",
    "class_accuracy_df = pd.DataFrame(list(class_accuracy.items()), columns=[\"Class\", \"Accuracy\"])\n",
    "\n",
    "misclassified_csv = os.path.join(csv_dir, \"misclassified_samples.csv\")\n",
    "low_confidence_csv = os.path.join(csv_dir, \"low_confidence_samples.csv\")\n",
    "class_accuracy_csv = os.path.join(csv_dir, \"classwise_accuracy.csv\")\n",
    "\n",
    "misclassified_df.to_csv(misclassified_csv, index=False)\n",
    "low_confidence_df.to_csv(low_confidence_csv, index=False)\n",
    "class_accuracy_df.to_csv(class_accuracy_csv, index=False)\n",
    "\n",
    "print(f\"\\nğŸ“„ ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ì˜¤ë‹µì´ ë§ì€ ìƒ˜í”Œ ì €ì¥ë¨: {misclassified_csv}\")\n",
    "print(f\"ğŸ“„ ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ í™•ì‹ ë„ê°€ ë‚®ì€ ìƒ˜í”Œ ì €ì¥ë¨: {low_confidence_csv}\")\n",
    "print(f\"ğŸ“„ í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì €ì¥ë¨: {class_accuracy_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
