{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‚¬ìš© ì¤‘ì¸ GPU: Quadro RTX 4000\n",
      "\n",
      "ğŸ“¢ Training Started! Logging Every Epoch:\n",
      "\n",
      "\n",
      "ğŸ“¢ Epoch [1/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.7248 | ğŸ“‰ Train Loss: 0.8655 | ğŸ¯ Valid Accuracy: 0.7976 | ğŸ“‰ Valid Loss: 0.6001\n",
      "ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 1, Accuracy: 0.7976)\n",
      "\n",
      "ğŸ“¢ Epoch [2/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.8387 | ğŸ“‰ Train Loss: 0.4997 | ğŸ¯ Valid Accuracy: 0.8164 | ğŸ“‰ Valid Loss: 0.6194\n",
      "ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 2, Accuracy: 0.8164)\n",
      "\n",
      "ğŸ“¢ Epoch [3/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.8801 | ğŸ“‰ Train Loss: 0.3607 | ğŸ¯ Valid Accuracy: 0.8463 | ğŸ“‰ Valid Loss: 0.5667\n",
      "ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 3, Accuracy: 0.8463)\n",
      "\n",
      "ğŸ“¢ Epoch [4/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9136 | ğŸ“‰ Train Loss: 0.2660 | ğŸ¯ Valid Accuracy: 0.8319 | ğŸ“‰ Valid Loss: 0.5880\n",
      "\n",
      "ğŸ“¢ Epoch [5/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9278 | ğŸ“‰ Train Loss: 0.2143 | ğŸ¯ Valid Accuracy: 0.8485 | ğŸ“‰ Valid Loss: 0.5894\n",
      "ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 5, Accuracy: 0.8485)\n",
      "\n",
      "ğŸ“¢ Epoch [6/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9366 | ğŸ“‰ Train Loss: 0.1921 | ğŸ¯ Valid Accuracy: 0.8561 | ğŸ“‰ Valid Loss: 0.5549\n",
      "ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 6, Accuracy: 0.8561)\n",
      "\n",
      "ğŸ“¢ Epoch [7/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9444 | ğŸ“‰ Train Loss: 0.1696 | ğŸ¯ Valid Accuracy: 0.8485 | ğŸ“‰ Valid Loss: 0.5791\n",
      "\n",
      "ğŸ“¢ Epoch [8/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9505 | ğŸ“‰ Train Loss: 0.1493 | ğŸ¯ Valid Accuracy: 0.8510 | ğŸ“‰ Valid Loss: 0.5861\n",
      "\n",
      "ğŸ“¢ Epoch [9/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9624 | ğŸ“‰ Train Loss: 0.1136 | ğŸ¯ Valid Accuracy: 0.8615 | ğŸ“‰ Valid Loss: 0.5668\n",
      "ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 9, Accuracy: 0.8615)\n",
      "\n",
      "ğŸ“¢ Epoch [10/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9569 | ğŸ“‰ Train Loss: 0.1295 | ğŸ¯ Valid Accuracy: 0.8438 | ğŸ“‰ Valid Loss: 0.7044\n",
      "\n",
      "ğŸ“¢ Epoch [11/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9575 | ğŸ“‰ Train Loss: 0.1263 | ğŸ¯ Valid Accuracy: 0.8496 | ğŸ“‰ Valid Loss: 0.6486\n",
      "\n",
      "ğŸ“¢ Epoch [12/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9711 | ğŸ“‰ Train Loss: 0.0910 | ğŸ¯ Valid Accuracy: 0.8449 | ğŸ“‰ Valid Loss: 0.6967\n",
      "\n",
      "ğŸ“¢ Epoch [13/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9638 | ğŸ“‰ Train Loss: 0.1069 | ğŸ¯ Valid Accuracy: 0.8355 | ğŸ“‰ Valid Loss: 0.7959\n",
      "\n",
      "ğŸ“¢ Epoch [14/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9646 | ğŸ“‰ Train Loss: 0.1071 | ğŸ¯ Valid Accuracy: 0.8420 | ğŸ“‰ Valid Loss: 0.7369\n",
      "\n",
      "ğŸ“¢ Epoch [15/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9639 | ğŸ“‰ Train Loss: 0.1106 | ğŸ¯ Valid Accuracy: 0.8532 | ğŸ“‰ Valid Loss: 0.6535\n",
      "\n",
      "ğŸ“¢ Epoch [16/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9730 | ğŸ“‰ Train Loss: 0.0872 | ğŸ¯ Valid Accuracy: 0.8521 | ğŸ“‰ Valid Loss: 0.6722\n",
      "\n",
      "ğŸ“¢ Epoch [17/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9772 | ğŸ“‰ Train Loss: 0.0712 | ğŸ¯ Valid Accuracy: 0.8391 | ğŸ“‰ Valid Loss: 0.6864\n",
      "\n",
      "ğŸ“¢ Epoch [18/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9756 | ğŸ“‰ Train Loss: 0.0783 | ğŸ¯ Valid Accuracy: 0.8344 | ğŸ“‰ Valid Loss: 0.7220\n",
      "\n",
      "ğŸ“¢ Epoch [19/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9773 | ğŸ“‰ Train Loss: 0.0696 | ğŸ¯ Valid Accuracy: 0.8492 | ğŸ“‰ Valid Loss: 0.7497\n",
      "\n",
      "ğŸ“¢ Epoch [20/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9750 | ğŸ“‰ Train Loss: 0.0788 | ğŸ¯ Valid Accuracy: 0.8391 | ğŸ“‰ Valid Loss: 0.7786\n",
      "\n",
      "ğŸ“¢ Epoch [21/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9709 | ğŸ“‰ Train Loss: 0.0874 | ğŸ¯ Valid Accuracy: 0.8481 | ğŸ“‰ Valid Loss: 0.7458\n",
      "\n",
      "ğŸ“¢ Epoch [22/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9770 | ğŸ“‰ Train Loss: 0.0722 | ğŸ¯ Valid Accuracy: 0.8618 | ğŸ“‰ Valid Loss: 0.7098\n",
      "ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 22, Accuracy: 0.8618)\n",
      "\n",
      "ğŸ“¢ Epoch [23/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9763 | ğŸ“‰ Train Loss: 0.0726 | ğŸ¯ Valid Accuracy: 0.8521 | ğŸ“‰ Valid Loss: 0.7124\n",
      "\n",
      "ğŸ“¢ Epoch [24/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9802 | ğŸ“‰ Train Loss: 0.0658 | ğŸ¯ Valid Accuracy: 0.8434 | ğŸ“‰ Valid Loss: 0.7442\n",
      "\n",
      "ğŸ“¢ Epoch [25/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9853 | ğŸ“‰ Train Loss: 0.0490 | ğŸ¯ Valid Accuracy: 0.8510 | ğŸ“‰ Valid Loss: 0.7611\n",
      "\n",
      "ğŸ“¢ Epoch [26/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9823 | ğŸ“‰ Train Loss: 0.0549 | ğŸ¯ Valid Accuracy: 0.8330 | ğŸ“‰ Valid Loss: 0.8016\n",
      "\n",
      "ğŸ“¢ Epoch [27/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9764 | ğŸ“‰ Train Loss: 0.0722 | ğŸ¯ Valid Accuracy: 0.8247 | ğŸ“‰ Valid Loss: 0.8272\n",
      "\n",
      "ğŸ“¢ Epoch [28/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9718 | ğŸ“‰ Train Loss: 0.0853 | ğŸ¯ Valid Accuracy: 0.8521 | ğŸ“‰ Valid Loss: 0.7156\n",
      "\n",
      "ğŸ“¢ Epoch [29/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9839 | ğŸ“‰ Train Loss: 0.0488 | ğŸ¯ Valid Accuracy: 0.8575 | ğŸ“‰ Valid Loss: 0.7046\n",
      "\n",
      "ğŸ“¢ Epoch [30/30] ì‹œì‘\n",
      "   ğŸ¯ Train Accuracy: 0.9826 | ğŸ“‰ Train Loss: 0.0548 | ğŸ¯ Valid Accuracy: 0.8301 | ğŸ“‰ Valid Loss: 0.8073\n",
      "ğŸ“„ ìµœê³ ì˜ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: C:/Users/user/OneDrive/Desktop/Resnet182-real/model\\final_category_model.pth\n",
      "ğŸ“„ í•™ìŠµ ê²°ê³¼ CSV ì €ì¥ë¨: C:/Users/user/OneDrive/Desktop/Resnet182-real/csv\\category_classification_results.csv\n",
      "âœ… í•™ìŠµ ì¢…ë£Œ! ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í•œ Epoch: 22, ğŸ¯ Train Accuracy: 0.9770, ğŸ“‰ Train Loss: 0.0722, ğŸ¯ Valid Accuracy: 0.8618, ğŸ“‰ Valid Loss: 0.7098\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "import random\n",
    "\n",
    "# âœ… GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"âœ… ì‚¬ìš© ì¤‘ì¸ GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# âœ… ê²½ë¡œ ì„¤ì •\n",
    "data_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/data/category_data\"\n",
    "model_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/model\"\n",
    "csv_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/csv\"\n",
    "json_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/json\"  # âœ… JSON ì €ì¥ ê²½ë¡œ\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "os.makedirs(json_dir, exist_ok=True)  # âœ… JSON ì €ì¥ í´ë” ìƒì„±\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¶„í•  ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ\n",
    "train_indices_path = os.path.join(json_dir, \"model1train.json\")\n",
    "val_indices_path = os.path.join(json_dir, \"model1val.json\")\n",
    "test_indices_path = os.path.join(json_dir, \"model1test.json\")\n",
    "\n",
    "# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# âœ… ë°ì´í„° ì „ì²˜ë¦¬\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "# âœ… ê° í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ë¶„ë°°\n",
    "class_indices = {cls: [] for cls in dataset.class_to_idx.values()}\n",
    "\n",
    "for idx, (path, label) in enumerate(dataset.samples):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# âœ… ê° í´ë˜ìŠ¤ë³„ë¡œ 7:2:1 ë¹„ìœ¨ë¡œ ë¶„í•  (í•™ìŠµ:ê²€ì¦:í…ŒìŠ¤íŠ¸)\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "\n",
    "for indices in class_indices.values():\n",
    "    random.shuffle(indices)  # ëœë¤ ì…”í”Œ\n",
    "    num_total = len(indices)\n",
    "    num_train = int(num_total * 0.7)\n",
    "    num_val = int(num_total * 0.2)\n",
    "    num_test = num_total - num_train - num_val\n",
    "\n",
    "    train_indices.extend(indices[:num_train])\n",
    "    val_indices.extend(indices[num_train:num_train+num_val])\n",
    "    test_indices.extend(indices[num_train+num_val:])\n",
    "\n",
    "# âœ… ë¶„í• ëœ ì¸ë±ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(train_indices_path, \"w\") as f:\n",
    "    json.dump(train_indices, f)\n",
    "with open(val_indices_path, \"w\") as f:\n",
    "    json.dump(val_indices, f)\n",
    "with open(test_indices_path, \"w\") as f:\n",
    "    json.dump(test_indices, f)\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¶„í• \n",
    "dataset_train = Subset(dataset, train_indices)\n",
    "dataset_val = Subset(dataset, val_indices)\n",
    "dataset_test = Subset(dataset, test_indices)\n",
    "\n",
    "# âœ… ë°ì´í„° ë¡œë” ì„¤ì •\n",
    "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# âœ… EfficientNet-B0 ëª¨ë¸ ì •ì˜\n",
    "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(dataset.classes))\n",
    "model.to(device)\n",
    "\n",
    "# âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# âœ… í•™ìŠµ ë£¨í”„\n",
    "best_valid_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "csv_path = os.path.join(csv_dir, \"category_classification_results.csv\")\n",
    "train_losses, valid_losses, train_accuracies, valid_accuracies = [], [], [], []\n",
    "\n",
    "print(\"\\nğŸ“¢ Training Started! Logging Every Epoch:\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nğŸ“¢ Epoch [{epoch+1}/{EPOCHS}] ì‹œì‘\")\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    total_train = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_loss /= total_train\n",
    "    train_accuracy = train_correct / total_train\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # âœ… ê²€ì¦ ë‹¨ê³„\n",
    "    model.eval()\n",
    "    valid_loss, valid_correct = 0, 0\n",
    "    total_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "            valid_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total_valid += labels.size(0)\n",
    "\n",
    "    valid_loss /= total_valid\n",
    "    valid_accuracy = valid_correct / total_valid\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "    print(f\"   ğŸ¯ Train Accuracy: {train_accuracy:.4f} | ğŸ“‰ Train Loss: {train_loss:.4f} | \"\n",
    "        f\"ğŸ¯ Valid Accuracy: {valid_accuracy:.4f} | ğŸ“‰ Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    if valid_accuracy > best_valid_accuracy:\n",
    "        best_valid_accuracy = valid_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        best_model_path = os.path.join(model_dir, \"best_category_model.pth\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch {epoch+1}, Accuracy: {best_valid_accuracy:.4f})\")\n",
    "\n",
    "# âœ… ìµœê³ ì˜ ê²€ì¦ ì •í™•ë„ë¥¼ ê¸°ë¡í•œ ì—í¬í¬ ëª¨ë¸ ì €ì¥\n",
    "model_save_path = os.path.join(model_dir, \"final_category_model.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"ğŸ“„ ìµœê³ ì˜ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_save_path}\")\n",
    "\n",
    "# âœ… ê²°ê³¼ CSV ì €ì¥\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, len(train_losses) + 1),\n",
    "    'Train Loss': train_losses,\n",
    "    'Valid Loss': valid_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Valid Accuracy': valid_accuracies\n",
    "})\n",
    "csv_path = os.path.join(csv_dir, \"category_classification_results.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"ğŸ“„ í•™ìŠµ ê²°ê³¼ CSV ì €ì¥ë¨: {csv_path}\")\n",
    "\n",
    "# âœ… ê²€ì¦ ì •í™•ë„ê°€ ê°€ì¥ ë†’ì€ ì—í¬í¬ì˜ ëª¨ë“  ì •ë³´ ì¶œë ¥\n",
    "print(f\"âœ… í•™ìŠµ ì¢…ë£Œ! ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í•œ Epoch: {best_epoch}, ğŸ¯ Train Accuracy: {train_accuracies[best_epoch-1]:.4f}, ğŸ“‰ Train Loss: {train_losses[best_epoch-1]:.4f}, ğŸ¯ Valid Accuracy: {best_valid_accuracy:.4f}, ğŸ“‰ Valid Loss: {valid_losses[best_epoch-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CUDA Available: True\n",
      "âœ… ì‚¬ìš© ì¤‘ì¸ GPU: Quadro RTX 4000\n",
      "âœ… Train ë°ì´í„°ì…‹ ì •í™•ë„: 0.9954\n",
      "âœ… Valid ë°ì´í„°ì…‹ ì •í™•ë„: 0.8618\n",
      "âœ… Test ë°ì´í„°ì…‹ ì •í™•ë„: 0.8734\n",
      "ğŸ“„ í‰ê°€ ê²°ê³¼ CSV ì €ì¥ë¨: C:/Users/user/OneDrive/Desktop/Resnet182-real/csv\\category_classification_evaluation.csv\n",
      "ğŸ“„ ìƒì„¸ í‰ê°€ ê²°ê³¼ ì €ì¥ë¨: C:/Users/user/OneDrive/Desktop/Resnet182-real/csv\\detailed_classification_results.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"âœ… CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ… ì‚¬ìš© ì¤‘ì¸ GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# âœ… ê²½ë¡œ ì„¤ì •\n",
    "data_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/data/category_data\"\n",
    "model_path = r\"C:\\Users\\user\\OneDrive\\Desktop\\Resnet182-real\\model\\best_category_model.pth\"\n",
    "csv_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/csv\"\n",
    "json_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/json\"  # âœ… JSON ì €ì¥ ê²½ë¡œ\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "# âœ… JSONì—ì„œ ì¸ë±ìŠ¤ ë¡œë“œ\n",
    "def load_indices(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_indices = load_indices(os.path.join(json_dir, \"model1train.json\"))\n",
    "val_indices = load_indices(os.path.join(json_dir, \"model1val.json\"))\n",
    "test_indices = load_indices(os.path.join(json_dir, \"model1test.json\"))\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¶„í• \n",
    "dataset_train = Subset(dataset, train_indices)\n",
    "dataset_val = Subset(dataset, val_indices)\n",
    "dataset_test = Subset(dataset, test_indices)\n",
    "\n",
    "# âœ… ë°ì´í„° ë¡œë” ì„¤ì •\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset_val, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, len(dataset.classes))\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "def evaluate_model(loader, dataset_name):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = outputs.argmax(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                image_idx = idx * loader.batch_size + i\n",
    "                pred_label = dataset.classes[predicted[i].item()]\n",
    "                true_label = dataset.classes[labels[i].item()]\n",
    "                if pred_label == true_label:\n",
    "                    results.append(f\"[âœ… ì •ë‹µ] ì´ë¯¸ì§€ {image_idx}: ì˜ˆì¸¡={pred_label}, ì‹¤ì œ={true_label}\")\n",
    "                else:\n",
    "                    results.append(f\"[âŒ ì˜¤ë‹µ] ì´ë¯¸ì§€ {image_idx}: ì˜ˆì¸¡={pred_label}, ì‹¤ì œ={true_label}\")\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"âœ… {dataset_name} ë°ì´í„°ì…‹ ì •í™•ë„: {accuracy:.4f}\")\n",
    "    return accuracy, results\n",
    "\n",
    "# âœ… í‰ê°€ ìˆ˜í–‰\n",
    "train_accuracy, train_results = evaluate_model(train_loader, \"Train\")\n",
    "valid_accuracy, valid_results = evaluate_model(valid_loader, \"Valid\")\n",
    "test_accuracy, test_results = evaluate_model(test_loader, \"Test\")\n",
    "\n",
    "# âœ… ì •í™•ë„ ë° ë¦¬ì½œ ê°’ CSV ì €ì¥\n",
    "df = pd.DataFrame({\n",
    "    'Dataset': ['Train', 'Valid', 'Test'],\n",
    "    'Accuracy': [train_accuracy, valid_accuracy, test_accuracy]\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(csv_dir, \"category_classification_evaluation.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"ğŸ“„ í‰ê°€ ê²°ê³¼ CSV ì €ì¥ë¨: {csv_path}\")\n",
    "\n",
    "# âœ… ìƒì„¸ ê²°ê³¼ ì €ì¥\n",
    "detail_results_path = os.path.join(csv_dir, \"detailed_classification_results.txt\")\n",
    "with open(detail_results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(train_results + valid_results + test_results))\n",
    "print(f\"ğŸ“„ ìƒì„¸ í‰ê°€ ê²°ê³¼ ì €ì¥ë¨: {detail_results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‚¬ìš© ì¤‘ì¸ GPU: Quadro RTX 4000\n",
      "âœ… Train ë°ì´í„°ì…‹ ì •í™•ë„: 0.9954\n",
      "âœ… Valid ë°ì´í„°ì…‹ ì •í™•ë„: 0.8618\n",
      "âœ… Test ë°ì´í„°ì…‹ ì •í™•ë„: 0.8734\n",
      "ğŸ“„ í‹€ë¦° ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (train, valid, test): C:/Users/user/OneDrive/Desktop/Resnet182-real/wrong_images\n",
      "ğŸ“„ í‰ê°€ ê²°ê³¼ CSV ì €ì¥ë¨: C:/Users/user/OneDrive/Desktop/Resnet182-real/csv\\category_classification_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# âœ… GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"âœ… ì‚¬ìš© ì¤‘ì¸ GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# âœ… ê²½ë¡œ ì„¤ì •\n",
    "data_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/data/category_data\"\n",
    "model_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/model\"\n",
    "csv_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/csv\"\n",
    "json_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/json\"\n",
    "wrong_images_dir = \"C:/Users/user/OneDrive/Desktop/Resnet182-real/wrong_images\"  # í‹€ë¦° ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ê²½ë¡œ\n",
    "\n",
    "# ìƒˆë¡œìš´ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(wrong_images_dir, exist_ok=True)\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "# âœ… JSONì—ì„œ ì¸ë±ìŠ¤ ë¡œë“œ\n",
    "def load_indices(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_indices = load_indices(os.path.join(json_dir, \"model1train.json\"))\n",
    "val_indices = load_indices(os.path.join(json_dir, \"model1val.json\"))\n",
    "test_indices = load_indices(os.path.join(json_dir, \"model1test.json\"))\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¶„í• \n",
    "dataset_train = Subset(dataset, train_indices)\n",
    "dataset_val = Subset(dataset, val_indices)\n",
    "dataset_test = Subset(dataset, test_indices)\n",
    "\n",
    "# âœ… ë°ì´í„° ë¡œë” ì„¤ì •\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset_val, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_path = os.path.join(model_dir, \"best_category_model.pth\")  # ë˜ëŠ” \"final_category_model.pth\"\n",
    "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, len(dataset.classes))\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "def evaluate_model(loader, dataset_name):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    wrong_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = outputs.argmax(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì •ë‹µê³¼ ì˜ˆì¸¡ê°’ ë¹„êµ\n",
    "            for i in range(len(labels)):\n",
    "                img_path = loader.dataset.dataset.samples[idx * loader.batch_size + i][0]\n",
    "                true_label = dataset.classes[labels[i].item()]\n",
    "                pred_label = dataset.classes[predicted[i].item()]\n",
    "\n",
    "                # í‹€ë¦° ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ\n",
    "                if true_label != pred_label:\n",
    "                    wrong_images.append((pred_label, true_label, img_path))\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"âœ… {dataset_name} ë°ì´í„°ì…‹ ì •í™•ë„: {accuracy:.4f}\")\n",
    "    return accuracy, wrong_images\n",
    "\n",
    "# âœ… ëª¨ë¸ í‰ê°€\n",
    "train_accuracy, train_wrong_images = evaluate_model(train_loader, \"Train\")\n",
    "valid_accuracy, valid_wrong_images = evaluate_model(valid_loader, \"Valid\")\n",
    "test_accuracy, test_wrong_images = evaluate_model(test_loader, \"Test\")\n",
    "\n",
    "# âœ… í‹€ë¦° ì´ë¯¸ì§€ë¥¼ ìƒˆë¡œìš´ í´ë”ì— ì €ì¥ (ì›ë³¸ í´ë” êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©° ë³µì‚¬)\n",
    "def save_wrong_images(wrong_images, base_dir, save_dir):\n",
    "    for pred_label, true_label, img_path in wrong_images:\n",
    "        # ì›ë³¸ í´ë”ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ\n",
    "        class_folder = os.path.basename(os.path.dirname(img_path))\n",
    "        new_dir = os.path.join(save_dir, class_folder)\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "        # ì´ë¯¸ì§€ ë³µì‚¬\n",
    "        shutil.copy(img_path, new_dir)\n",
    "\n",
    "# âœ… ê° ë°ì´í„°ì…‹ì—ì„œ í‹€ë¦° ì´ë¯¸ì§€ ì €ì¥\n",
    "save_wrong_images(train_wrong_images, data_dir, os.path.join(wrong_images_dir, \"train\"))\n",
    "save_wrong_images(valid_wrong_images, data_dir, os.path.join(wrong_images_dir, \"valid\"))\n",
    "save_wrong_images(test_wrong_images, data_dir, os.path.join(wrong_images_dir, \"test\"))\n",
    "\n",
    "print(f\"ğŸ“„ í‹€ë¦° ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (train, valid, test): {wrong_images_dir}\")\n",
    "\n",
    "# âœ… ê²°ê³¼ CSV ì €ì¥\n",
    "df = pd.DataFrame({\n",
    "    'Dataset': ['Train', 'Valid', 'Test'],\n",
    "    'Accuracy': [train_accuracy, valid_accuracy, test_accuracy]\n",
    "})\n",
    "\n",
    "csv_path = os.path.join(csv_dir, \"category_classification_evaluation.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"ğŸ“„ í‰ê°€ ê²°ê³¼ CSV ì €ì¥ë¨: {csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
