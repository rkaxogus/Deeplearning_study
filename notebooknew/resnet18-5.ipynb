{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "âœ… ëª¨ë“  ì œí’ˆ í´ë”ë¥¼ 'processed_data5'ë¡œ ì´ë™ ì™„ë£Œ!\n",
      "ì´ í´ë˜ìŠ¤ ê°œìˆ˜: 171\n",
      "í´ë˜ìŠ¤ ëª©ë¡: ['12X', '17.2', '4S ì™€ì´ë“œ', '6.3', 'A5', 'A7', 'A9', 'D1', 'IE', 'M1', 'P1', 'T ì½”ì–´', 'U.L', 'X ì½”ì–´', 'X5', 'protect', 'ê°€ë§ˆë³´ì½”', 'ê³ ìŠ¤íŠ¸', 'ê³ í‹€ëœë“œ', 'ê³¨ë“œí‚¤ìœ„', 'êµ¬ì•„ë°”', 'ê·¸ë‘ë² ë¥´í¬', 'ê·¸ëœì¦ˆë¹Œ ë ‰íƒ€ íƒ€í”„', 'ë‚˜ë¥´ì‹œìŠ¤ ë”', 'ë‚˜ë§ˆì¸ ', 'ë‚ ë¡œ', 'ë„¤ë·¸ë¼', 'ë„¤ìŠ¤íŠ¸', 'ë…¸ë‚˜ë”', 'ë‹ˆì•…', 'ë°í¬ì‰˜í„°', 'ë„í¬ë”', 'ë¼ì´ë”ìŠ¤', 'ë¼ì¸ìŠ¤í”¼ì—˜', 'ëœë“œë¡', 'ëœë“œë§ˆí¬', 'ëœë“œë¸Œë¦¬ì¦ˆ', 'ë ˆì´ì‚¬', 'ë ‰íƒ€', 'ë ‰íƒ€ íƒ€í”„', 'ë ‰íƒ€íƒ€í”„', 'ë¡œë§¨í‹±í„°í‹€', 'ë£¨ë‚˜', 'ë¦¬ë¹™ì‰˜', 'ë§ìŠ¤í‹´ë“œ', 'ë§ˆë¦¬ë‚˜', 'ë©€í‹°í‘ì…˜ ì•”ë§‰íƒ€í”„', 'ë©”ì‰¬ íƒ€í”„ìŠ¤í¬ë¦°', 'ëª½ê°€', 'ë¬¸ë¼ì´íŠ¸', 'ë¯¸ë¼í´ìŠ¤í¬ë¦°', 'ë¯¸ë¼í´íŒ¨ë°€ë¦¬', 'ë°”ë‘ì—ë¥´', 'ë°”ë‘ì—ë¥´ë”', 'ë°œí• ', 'ë±…ê°€ë“œ', 'ë³´ë‹ˆì•„ë˜', 'ë³´ìŠ¤', 'ë³¼íŠ¸', 'ë¸Œì´íƒ€í”„', 'ë¸”ë™ ì½”íŠ¸', 'ë¹„ë¬´ë¥´', 'ë¹„ë°” ë ‰íƒ€ íƒ€í”„', 'ë¹„ë°”ë”', 'ë¹„í‹°í˜¸ë¥¸', 'ë¹…', 'ë¹Œë¦¬ì§€', 'ìƒˆí„´ ì‰˜í„°', 'ìƒˆí„´ ì—ì–´í…íŠ¸', 'ìƒˆí„´2ë£¸', 'ì„¸ì´ëœë“œ', 'ì…°ì–´', 'ì†”ë¡œ', 'ì‰˜í„°G', 'ì‰˜í„°ë”', 'ìŠˆí¼íŒ°ë¦¬ìŠ¤', 'ìŠ¤íƒ€ì´ì¹´', 'ìŠ¤í…”ë¼ë¦¿ì§€', 'ì¬ë¸”ëŸ­', 'ì•„ê³ ë¼', 'ì•„ë¦¬ì—ìŠ¤', 'ì•„ìŠ¤ê°€ë¥´ë“œ', 'ì•„ìŠ¤í„°ë”', 'ì•„ìŠ¤íŠ¸ë¼ë”', 'ì•„ì¼€ë””ì•„', 'ì•„í€¼ë¼', 'ì•„í‹€ë¼ìŠ¤', 'ì•„í‹€ë€í‹±', 'ì•„í‹°ì¹´', 'ì•Œë½', 'ì•Œë² ë¥´ê²Œ', 'ì•Œë¹„ì˜¨', 'ì•ŒíŒŒ ë£¸', 'ì•ŒíŒŒì¸ë”', 'ì•ŒíŒŒì¸ë¼ì´íŠ¸', 'ì•Œí˜ì„', 'ì•¼ë¥¸ë¹„ë“œ', 'ì–´ë©”ë‹ˆí‹°ë”', 'ì—ë³¼ë£¨ì…˜ ë©”ì‰¬ì‰˜í„°', 'ì—ì–´ ë„í‚¹', 'ì—ì–´ ë¦¬ë¹™ ì‰˜í„°', 'ì—ì½” ì†Œìš¸ ì˜ˆì´ì˜ˆì´', 'ì˜ë¡œìš°ìŠ¤í†¤', 'ì˜¤ìš°ì¹˜', 'ì˜¤í† ë“€ì–¼íŒ”ë ˆìŠ¤', 'ì˜¤í† í•˜ìš°ìŠ¤', 'ì˜¤íŠ¸ë¼', 'ì˜¤íŒŒëŸ¬ìŠ¤', 'ì˜¤í”„ëœë“œ', 'ì™€ê°€ì•¼ë…¸', 'ì™€ì´ì¦ˆ íƒ€í”„ì‰˜', 'ì™€ì¼ë“œ í•„ë“œ ì˜¤ìŠ¤ì¹´', 'ì™€ì¼ë“œ í•„ë“œ ì˜¥íƒ€ê³¤', 'ì™€ì¼ë“œ í•„ë“œ í—¥ì‚¬ íƒ€í”„', 'ìš°ë‚˜', 'ìš°ë¥´ì‚¬', 'ìš°íŠ¸ê°€ë¥´ë“œ', 'ì›Œí„°ë©œë¡ ', 'ì›í„°ì¹˜', 'ì›¨ë”ë§ˆìŠ¤í„°', 'ì´ë„ˆí…íŠ¸', 'ì´ë“ ', 'ì´ì§€íŒ', 'ì¸ë””ì•„ë‚˜', 'ì¸ìŠ¤í„´íŠ¸ì—… 3p', 'ì¸ìŠ¤í„´íŠ¸ì—…ë”', 'ì œë“œ7', 'ì œì´ìŠ¤íƒ€', 'ì§€ì˜¤íŒ¨ìŠ¤', 'ì¹´ë¦¬', 'ì¹´ë°”ë‚˜', 'ì¹´ì´í…€', 'ì¹´í…íŠ¸', 'ì¹´í”„ë¦¬ì½˜', 'ìºìŠ¬', 'ìº¥ê±°ë£¨', 'ì»´íŒ©íŠ¸ ì›í„°ì¹˜', 'ì¼€ë¡ ', 'ì¼€íŒ”ë¡œ', 'ì½”íŠ¸í…íŠ¸', 'í¬ë¡œë…¸ìŠ¤', 'í¬ë¡œìŠ¤ ë¹…ì‰˜í„°', 'í´ë¼ìš°ë“œì—…', 'í´ë¼ìš°ë“œí”¼í¬', 'í‚¤ë…¸ì½”', 'íƒ€ìš°ë£¨ìŠ¤', 'íƒ ì €ë¦°', 'í„°í”„ë”', 'í„°í”„ìŠ¤í¬ë¦°', 'í„°í”„ì™€ì´ë“œë”', 'íˆ¬ë ', 'íˆ¬ì–´ë§', 'íˆ¬ì–´ë§ë”', 'íŠ¸ë¦¬ì˜¨', 'í‹°ì–´ë”', 'íŒŒë…¸ë¼ë§ˆ', 'íŒŒí‹°ìºë¹ˆ', 'íŒŒí”„ë¦¬ì¹´', 'íŒì—…', 'íŒ¨ìŠ¤ë¹…', 'í¼ì‹œí”½ì˜¤ì…˜', 'í°í”¼ì—˜', 'í¼ë¯€', 'í”Œë ˆì´ì–´', 'í•„ë“œí„°ë„', 'í•˜ìš°ìŠ¤', 'í•˜ì´ë¹„', 'í• ë€ë“œ', 'í—¥ì‚¬ íƒ€í”„', 'í—¬ë½ìŠ¤', 'íœ˜ë–¼']\n",
      "âœ… train5.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… val5.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… test5.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\classification3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\classification3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Epoch [1/500] | ğŸ“‰ Train Loss: 3.3700 | ğŸ¯ Train Accuracy: 34.2060 | ğŸ“‘ Valid Loss: 2.2753 | ğŸ¯ Valid Accuracy: 53.6799\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸ“Œ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 1, Accuracy: 53.6799) âœ…\n",
      "ğŸš€ Epoch [2/500] | ğŸ“‰ Train Loss: 1.9650 | ğŸ¯ Train Accuracy: 60.7665 | ğŸ“‘ Valid Loss: 1.7908 | ğŸ¯ Valid Accuracy: 62.0189\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸ“Œ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 2, Accuracy: 62.0189) âœ…\n",
      "ğŸš€ Epoch [3/500] | ğŸ“‰ Train Loss: 1.4303 | ğŸ¯ Train Accuracy: 70.2428 | ğŸ“‘ Valid Loss: 1.5358 | ğŸ¯ Valid Accuracy: 66.2728\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸ“Œ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 3, Accuracy: 66.2728) âœ…\n",
      "ğŸš€ Epoch [4/500] | ğŸ“‰ Train Loss: 1.1046 | ğŸ¯ Train Accuracy: 75.7067 | ğŸ“‘ Valid Loss: 1.4429 | ğŸ¯ Valid Accuracy: 67.4882\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸ“Œ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 4, Accuracy: 67.4882) âœ…\n",
      "ğŸš€ Epoch [5/500] | ğŸ“‰ Train Loss: 0.8820 | ğŸ¯ Train Accuracy: 80.0133 | ğŸ“‘ Valid Loss: 1.4026 | ğŸ¯ Valid Accuracy: 68.6361\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸ“Œ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 5, Accuracy: 68.6361) âœ…\n",
      "ğŸš€ Epoch [6/500] | ğŸ“‰ Train Loss: 0.7096 | ğŸ¯ Train Accuracy: 83.0393 | ğŸ“‘ Valid Loss: 1.3646 | ğŸ¯ Valid Accuracy: 68.9737\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸ“Œ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 6, Accuracy: 68.9737) âœ…\n",
      "ğŸš€ Epoch [7/500] | ğŸ“‰ Train Loss: 0.5911 | ğŸ¯ Train Accuracy: 85.4297 | ğŸ“‘ Valid Loss: 1.3616 | ğŸ¯ Valid Accuracy: 68.7373\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [8/500] | ğŸ“‰ Train Loss: 0.5030 | ğŸ¯ Train Accuracy: 86.9759 | ğŸ“‘ Valid Loss: 1.3740 | ğŸ¯ Valid Accuracy: 69.5138\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸ“Œ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch 8, Accuracy: 69.5138) âœ…\n",
      "ğŸš€ Epoch [9/500] | ğŸ“‰ Train Loss: 0.4440 | ğŸ¯ Train Accuracy: 88.2091 | ğŸ“‘ Valid Loss: 1.4236 | ğŸ¯ Valid Accuracy: 67.2856\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [10/500] | ğŸ“‰ Train Loss: 0.4101 | ğŸ¯ Train Accuracy: 88.4272 | ğŸ“‘ Valid Loss: 1.3908 | ğŸ¯ Valid Accuracy: 68.6698\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [11/500] | ğŸ“‰ Train Loss: 0.3906 | ğŸ¯ Train Accuracy: 88.6454 | ğŸ“‘ Valid Loss: 1.4327 | ğŸ¯ Valid Accuracy: 67.5219\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [12/500] | ğŸ“‰ Train Loss: 0.3698 | ğŸ¯ Train Accuracy: 89.0913 | ğŸ“‘ Valid Loss: 1.4772 | ğŸ¯ Valid Accuracy: 66.9142\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [13/500] | ğŸ“‰ Train Loss: 0.3451 | ğŸ¯ Train Accuracy: 89.4897 | ğŸ“‘ Valid Loss: 1.4761 | ğŸ¯ Valid Accuracy: 66.9818\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [14/500] | ğŸ“‰ Train Loss: 0.3365 | ğŸ¯ Train Accuracy: 89.2525 | ğŸ“‘ Valid Loss: 1.4657 | ğŸ¯ Valid Accuracy: 67.6232\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [15/500] | ğŸ“‰ Train Loss: 0.3257 | ğŸ¯ Train Accuracy: 89.4991 | ğŸ“‘ Valid Loss: 1.5235 | ğŸ¯ Valid Accuracy: 67.5557\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [16/500] | ğŸ“‰ Train Loss: 0.3125 | ğŸ¯ Train Accuracy: 89.7837 | ğŸ“‘ Valid Loss: 1.4809 | ğŸ¯ Valid Accuracy: 67.1168\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [17/500] | ğŸ“‰ Train Loss: 0.3005 | ğŸ¯ Train Accuracy: 89.8596 | ğŸ“‘ Valid Loss: 1.5003 | ğŸ¯ Valid Accuracy: 67.5219\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [18/500] | ğŸ“‰ Train Loss: 0.2921 | ğŸ¯ Train Accuracy: 89.8691 | ğŸ“‘ Valid Loss: 1.5447 | ğŸ¯ Valid Accuracy: 66.3065\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000100\n",
      "ğŸš€ Epoch [19/500] | ğŸ“‰ Train Loss: 0.2981 | ğŸ¯ Train Accuracy: 89.7363 | ğŸ“‘ Valid Loss: 1.5429 | ğŸ¯ Valid Accuracy: 66.9142\n",
      "Epoch 00019: reducing learning rate of group 0 to 5.0000e-05.\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš€ Epoch [20/500] | ğŸ“‰ Train Loss: 0.2401 | ğŸ¯ Train Accuracy: 90.7608 | ğŸ“‘ Valid Loss: 1.4852 | ğŸ¯ Valid Accuracy: 68.7711\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš€ Epoch [21/500] | ğŸ“‰ Train Loss: 0.2159 | ğŸ¯ Train Accuracy: 91.1023 | ğŸ“‘ Valid Loss: 1.5058 | ğŸ¯ Valid Accuracy: 68.6361\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš€ Epoch [22/500] | ğŸ“‰ Train Loss: 0.2146 | ğŸ¯ Train Accuracy: 91.1687 | ğŸ“‘ Valid Loss: 1.5095 | ğŸ¯ Valid Accuracy: 68.3660\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš€ Epoch [23/500] | ğŸ“‰ Train Loss: 0.2092 | ğŸ¯ Train Accuracy: 90.7703 | ğŸ“‘ Valid Loss: 1.4999 | ğŸ¯ Valid Accuracy: 68.4673\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš€ Epoch [24/500] | ğŸ“‰ Train Loss: 0.2054 | ğŸ¯ Train Accuracy: 91.0548 | ğŸ“‘ Valid Loss: 1.5315 | ğŸ¯ Valid Accuracy: 67.7245\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš€ Epoch [25/500] | ğŸ“‰ Train Loss: 0.2103 | ğŸ¯ Train Accuracy: 91.1781 | ğŸ“‘ Valid Loss: 1.5025 | ğŸ¯ Valid Accuracy: 68.5010\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš€ Epoch [26/500] | ğŸ“‰ Train Loss: 0.2031 | ğŸ¯ Train Accuracy: 91.0928 | ğŸ“‘ Valid Loss: 1.5292 | ğŸ¯ Valid Accuracy: 67.9608\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš€ Epoch [27/500] | ğŸ“‰ Train Loss: 0.2046 | ğŸ¯ Train Accuracy: 90.9220 | ğŸ“‘ Valid Loss: 1.5557 | ğŸ¯ Valid Accuracy: 67.8933\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš€ Epoch [28/500] | ğŸ“‰ Train Loss: 0.1993 | ğŸ¯ Train Accuracy: 91.1117 | ğŸ“‘ Valid Loss: 1.5607 | ğŸ¯ Valid Accuracy: 68.7036\n",
      "í˜„ì¬ í•™ìŠµë¥ : 0.000050\n",
      "ğŸš¨ ì¡°ê¸° ì¢…ë£Œ: 20 ì—í­ ë™ì•ˆ ê°œì„  ì—†ìŒ.\n",
      "ğŸ¯ ìµœì¢… ìµœê³  ê²€ì¦ ì •í™•ë„: 69.51%\n",
      "âœ… í•™ìŠµ ê¸°ë¡ ì €ì¥ ì™„ë£Œ: C:\\Users\\user\\OneDrive\\Desktop\\Resnet18-real\\csv\\í…íŠ¸ë¶„ë¥˜3.csv\n",
      "ğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì •í™•ë„: 68.84%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Subset  # [ë³€ê²½ë¨]: random_split ëŒ€ì‹  Subset ì‚¬ìš©\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np  # [ë³€ê²½ë¨]: numpy ì„í¬íŠ¸\n",
    "\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "data_dir = r\"C:\\Users\\user\\OneDrive\\Desktop\\Resnet18-real\\data\\brand_data5\"\n",
    "processed_data_dir = r\"C:\\Users\\user\\OneDrive\\Desktop\\Resnet18-real\\data\\processed_data5\"\n",
    "json_save_dir = r\"C:\\Users\\user\\OneDrive\\Desktop\\Resnet18-real\\json\"\n",
    "csv_save_dir = r\"C:\\Users\\user\\OneDrive\\Desktop\\Resnet18-real\\csv\"\n",
    "model_save_path = r\"C:\\Users\\user\\OneDrive\\Desktop\\Resnet18-real\\modelnew\\resnet18-5.pth\"\n",
    "\n",
    "# JSON, CSV ì €ì¥ í´ë” ìƒì„±\n",
    "os.makedirs(json_save_dir, exist_ok=True)\n",
    "os.makedirs(csv_save_dir, exist_ok=True)\n",
    "\n",
    "# ëª¨ë“  ë¸Œëœë“œ í´ë”ë¥¼ ìˆœíšŒí•˜ë©´ì„œ í•˜ìœ„ ì œí’ˆ í´ë” ì´ë™\n",
    "for brand in os.listdir(data_dir):\n",
    "    brand_path = os.path.join(data_dir, brand)\n",
    "    if os.path.isdir(brand_path):\n",
    "        for product in os.listdir(brand_path):\n",
    "            product_path = os.path.join(brand_path, product)\n",
    "            if os.path.isdir(product_path):\n",
    "                new_product_path = os.path.join(processed_data_dir, product)\n",
    "                os.makedirs(new_product_path, exist_ok=True)\n",
    "                for image in os.listdir(product_path):\n",
    "                    src = os.path.join(product_path, image)\n",
    "                    dst = os.path.join(new_product_path, image)\n",
    "                    shutil.copy(src, dst)\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ì œí’ˆ í´ë”ë¥¼ 'processed_data5'ë¡œ ì´ë™ ì™„ë£Œ!\")\n",
    "\n",
    "# ì´ë¯¸ì§€ ë³€í™˜ ì„¤ì •\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "dataset = datasets.ImageFolder(root=processed_data_dir, transform=transform)\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"ì´ í´ë˜ìŠ¤ ê°œìˆ˜: {num_classes}\")\n",
    "print(\"í´ë˜ìŠ¤ ëª©ë¡:\", dataset.classes)\n",
    "\n",
    "# [ë³€ê²½ë¨] ë°ì´í„°ì…‹ ë¶„í•  - í´ë˜ìŠ¤ë³„ë¡œ 7:2:1 ë¹„ìœ¨ë¡œ ë¶„í• \n",
    "# ê° í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "class_indices = {cls: [] for cls in range(num_classes)}\n",
    "for idx, (path, label) in enumerate(dataset.imgs):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "for label, indices in class_indices.items():\n",
    "    np.random.shuffle(indices)  # ì¸ë±ìŠ¤ ëœë¤ ì…”í”Œ\n",
    "    n = len(indices)\n",
    "    n_train = int(0.7 * n)\n",
    "    n_val = int(0.2 * n)\n",
    "    # ë‚˜ë¨¸ì§€ëŠ” í…ŒìŠ¤íŠ¸ì…‹ì— í• ë‹¹\n",
    "    train_indices.extend(indices[:n_train])\n",
    "    val_indices.extend(indices[n_train:n_train+n_val])\n",
    "    test_indices.extend(indices[n_train+n_val:])\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# [ë³€ê²½ë¨ ë]\n",
    "\n",
    "# JSON íŒŒì¼ë¡œ ë°ì´í„°ì…‹ ì €ì¥\n",
    "def save_json(subset_dataset, dataset_name):\n",
    "    data_list = [{\"image_path\": subset_dataset.dataset.imgs[i][0], \"label\": int(subset_dataset.dataset.imgs[i][1])} \n",
    "                for i in subset_dataset.indices]\n",
    "    json_path = os.path.join(json_save_dir, f\"{dataset_name}.json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_list, f, indent=4)\n",
    "    print(f\"âœ… {dataset_name}.json ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "save_json(train_dataset, \"train5\")\n",
    "save_json(val_dataset, \"val5\")\n",
    "save_json(test_dataset, \"test5\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë” ì„¤ì •\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì • (ResNet18)\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™” í•¨ìˆ˜ ì„¤ì •\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# [ì¶”ê°€ë¨] Learning rate scheduler (ReduceLROnPlateau ì‚¬ìš© - ê²€ì¦ ì •í™•ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµë¥  ì¡°ì ˆ)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# í•™ìŠµ ê¸°ë¡ ì €ì¥ìš© DataFrame\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "# [ì¶”ê°€ë¨] Early Stopping ì„¤ì • ë³€ìˆ˜ (ê°œì„ ì´ ì—†ì„ ì‹œ ì¡°ê¸° ì¢…ë£Œ)\n",
    "early_stop_patience = 20\n",
    "no_improve_count = 0\n",
    "\n",
    "# [ì¶”ê°€ë¨] ë¡œê·¸ íŒŒì¼ ìƒì„± (í•™ìŠµ ê¸°ë¡ ì¶”ê°€ ì €ì¥)\n",
    "log_file = os.path.join(csv_save_dir, \"train_log5.txt\")\n",
    "with open(log_file, \"w\") as lf:\n",
    "    lf.write(\"Epoch,Train Loss,Train Acc,Val Loss,Val Acc,Learning Rate\\n\")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
    "    global no_improve_count  # [ì¶”ê°€ë¨] early stopping ì¹´ìš´íŠ¸ ë³€ìˆ˜ ì‚¬ìš©\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_train += predicted.eq(labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_val += predicted.eq(labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        val_acc = 100 * correct_val / total_val\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        history[\"epoch\"].append(epoch + 1)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"ğŸš€ Epoch [{epoch+1}/{num_epochs}] | ğŸ“‰ Train Loss: {train_loss:.4f} | ğŸ¯ Train Accuracy: {train_acc:.4f} | ğŸ“‘ Valid Loss: {val_loss:.4f} | ğŸ¯ Valid Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # [ì¶”ê°€ë¨] Learning rate scheduler step (ê²€ì¦ ì •í™•ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµë¥  ì¡°ì •)\n",
    "        scheduler.step(val_acc)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"í˜„ì¬ í•™ìŠµë¥ : {current_lr:.6f}\")\n",
    "\n",
    "        # [ì¶”ê°€ë¨] ë¡œê·¸ íŒŒì¼ì— í•™ìŠµ ê¸°ë¡ ì¶”ê°€ ì €ì¥\n",
    "        with open(log_file, \"a\") as lf:\n",
    "            lf.write(f\"{epoch+1},{train_loss:.4f},{train_acc:.4f},{val_loss:.4f},{val_acc:.4f},{current_lr:.6f}\\n\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"ğŸ“Œ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨! (Epoch {epoch+1}, Accuracy: {val_acc:.4f}) âœ…\")\n",
    "            no_improve_count = 0  # [ì¶”ê°€ë¨] ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”\n",
    "        else:\n",
    "            no_improve_count += 1  # [ì¶”ê°€ë¨] ê°œì„ ë˜ì§€ ì•ŠìŒ\n",
    "\n",
    "        # [ì¶”ê°€ë¨] Early Stopping ì¡°ê±´ í™•ì¸ (ê°œì„ ì´ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ)\n",
    "        if no_improve_count >= early_stop_patience:\n",
    "            print(f\"ğŸš¨ ì¡°ê¸° ì¢…ë£Œ: {early_stop_patience} ì—í­ ë™ì•ˆ ê°œì„  ì—†ìŒ.\")\n",
    "            break\n",
    "\n",
    "    print(f\"ğŸ¯ ìµœì¢… ìµœê³  ê²€ì¦ ì •í™•ë„: {best_acc:.2f}%\")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=500)\n",
    "\n",
    "# í•™ìŠµ ê³¼ì • CSVë¡œ ì €ì¥\n",
    "df = pd.DataFrame(history)\n",
    "csv_path = os.path.join(csv_save_dir, \"í…íŠ¸ë¶„ë¥˜3.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… í•™ìŠµ ê¸°ë¡ ì €ì¥ ì™„ë£Œ: {csv_path}\")\n",
    "\n",
    "# ìµœì  ëª¨ë¸ ë¡œë“œ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "correct_test, total_test = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct_test += predicted.eq(labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "test_acc = 100 * correct_test / total_test\n",
    "print(f\"ğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì •í™•ë„: {test_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
